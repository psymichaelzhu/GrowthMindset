{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "import collections\n",
    "import os\n",
    "import csv\n",
    "\n",
    "__demo=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP_Environment:\n",
    "    '''\n",
    "    A Markov Decision Process (MDP) Environment consists of:\n",
    "    \n",
    "    - states: [attribute] a list of state names.\n",
    "    - actions: [attribute] a list of action names.\n",
    "    - reward_table r(s,a): A 2D matrix that stores the reward obtained when action 'a' is performed in state 's'.\n",
    "    - transition_table p(s'|s,a): [attribute] a 3D matrix which stores the transition probability of going from state 's' to state 's'' given action 'a'.\n",
    "\n",
    "    They provided a complete description of the MDP environment dynamics.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, states, actions, reward_table, transition_table, session_length):\n",
    "        '''\n",
    "        Initializes the MDP Environment with states, actions, a reward table, and a transition table.\n",
    "        \n",
    "        Args:\n",
    "        - states: List of state names.\n",
    "        - actions: List of action names.\n",
    "        - reward_table: 2D array for rewards where reward_table[a,s] is the reward for action a in state s.\n",
    "        - transition_table: 3D array for transitions where transition_table[a,s1,s2] is the probability of going from s1 to s2 with action a.\n",
    "        - session_length: The total number of time steps in which the agent can interact with the environment.\n",
    "        '''\n",
    "        self.states = np.array(states) \n",
    "        self.actions = np.array(actions)\n",
    "        self.reward_table = np.array(reward_table)\n",
    "        self.transition_table = np.array(transition_table)\n",
    "        self.session_length = session_length\n",
    "\n",
    "        self.parameter={\n",
    "            \"session_length\": session_length\n",
    "        }\n",
    "    \n",
    "    def build_inner_model(self):\n",
    "        '''\n",
    "        Agent's belief about the environment.\n",
    "        \n",
    "        Returns:\n",
    "        - A deep copy of the environment instance.\n",
    "        '''\n",
    "        model = copy.deepcopy(self)\n",
    "        return model\n",
    "    \n",
    "\n",
    "\n",
    "def demo():\n",
    "    states = ['s1', 's2', 's3']\n",
    "    actions = ['a1', 'a2']\n",
    "    reward_table = [[10, 5, 2],  # rewards for action a1 in state s2: 5\n",
    "                    [8, 6, 1]]\n",
    "    transition_table = [\n",
    "        [[0.8, 0.2, 0.0],  # probability of transitioning from s1 to s2 when conducting action a1: 0.2\n",
    "        [0.1, 0.9, 0.0],  \n",
    "        [0.0, 0.1, 0.9]],\n",
    "\n",
    "        [[0.7, 0.3, 0.0],  \n",
    "        [0.2, 0.7, 0.1],  \n",
    "        [0.0, 0.3, 0.7]] ]\n",
    "    session_length = 100\n",
    "    env = MDP_Environment(states, actions, reward_table, transition_table, session_length)\n",
    "    print(f\"reward table: \\n{env.reward_table}\")\n",
    "    print(type(env.reward_table))\n",
    "    print(f\"transition probability table: \\n{env.transition_table}\")   \n",
    "    print(f\"Reward for taking action 'a1' in 's1' : {env.reward_table[0, 1]}\")#should be 5\n",
    "    print(f\"Probability of transitioning from 's1' to 's2' through action 'a1': {env.transition_table[0,0,1]}\")#should be 0.2\n",
    "\n",
    "if __demo:\n",
    "    demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mindset_Environment(MDP_Environment):\n",
    "    '''\n",
    "    A specialized MDP environment that characterizes the environment malleability \n",
    "        environment malleability: the probability of transitioning from one state to a better state through appropriate actions.\n",
    "\n",
    "    Parameters:\n",
    "    - state_num: Number of states in the environment.\n",
    "    - reward_baseline: The baseline reward for state-independent actions.\n",
    "    - reward_increment: The incremental reward for state-dependent actions.\n",
    "    - malleability: The probability of transitioning to a higher state for the state-dependent action.\n",
    "    - session_length: The total number of time steps in which the agent can interact with the environment.\n",
    "    '''\n",
    "    def __init__(self, state_num=3, reward_baseline=2, reward_increment=1, malleability=1.0, session_length=100):\n",
    "        states = [f\"s{i+1}\" for i in range(state_num)]\n",
    "        actions = [\"A_dependent\", \"A_independent\"]\n",
    "\n",
    "        middle_state_index = state_num // 2\n",
    "        # Reward table\n",
    "        reward_table = [\n",
    "            [reward_baseline + (i - middle_state_index) * reward_increment for i in range(state_num)], # Reward for state-related action: a positive linear function of the state index, with 'reward_baseline' at the middle state index, and 'reward_increment' as the reward increment.\n",
    "            [reward_baseline] * state_num  # Reward for state-independent action: a constant 'reward_baseline'\n",
    "        ]\n",
    "\n",
    "        # Transition table: \n",
    "        transition_table = [\n",
    "            self.generate_dependent_transition(state_num, malleability),# Transition for state-dependent action: The probability of transition from a lower state to a higher state is 'malleability', while the probability of maintaining the current state is '(1-malleability)'.\n",
    "            np.identity(state_num)  #Transition for state-independent action: always maintaining the current state (probability is 1)\n",
    "        ]\n",
    "\n",
    "        super().__init__(states, actions, reward_table, transition_table, session_length)\n",
    "\n",
    "        #record environment parameter\n",
    "        self.parameter={\n",
    "            \"state_num\": state_num,\n",
    "            \"reward_baseline\": reward_baseline,\n",
    "            \"reward_increment\": reward_increment,\n",
    "            \"malleability\": malleability,\n",
    "            \"session_length\": session_length\n",
    "        }\n",
    "    \n",
    "\n",
    "    def generate_dependent_transition(self, state_num, malleability):\n",
    "        '''\n",
    "        Generates a transition matrix for the state-dependent action with a given 'malleability'.\n",
    "        \n",
    "        Args:\n",
    "        - state_num: Number of states.\n",
    "        - malleability: Probability of transitioning to the next higher state.\n",
    "\n",
    "        Returns:\n",
    "        - transition_matrix: A matrix showing probabilities of transitions between states.\n",
    "        '''\n",
    "        transition_matrix = np.zeros((state_num, state_num))\n",
    "        \n",
    "        # Last state stays the same\n",
    "        transition_matrix[state_num - 1, state_num - 1] = 1.0\n",
    "        \n",
    "        # For other states: transition to next state with 'malleability', stay with '1 - malleability'\n",
    "        transition_matrix[:state_num - 1, 1:state_num ] += np.identity(state_num - 1) * malleability\n",
    "        transition_matrix[:state_num - 1, :state_num - 1] += np.identity(state_num - 1) * (1 - malleability)\n",
    "\n",
    "        return transition_matrix\n",
    "    \n",
    "    def build_inner_model(self, malleability):\n",
    "        '''\n",
    "        This is the internal representation of agent for environmental dynamics. \n",
    "        Return a deep copy of the environment and update the transition matrix of dependent actions based on \"malleability\".\n",
    "\n",
    "        Args:\n",
    "        - malleability: Probability for transitioning to a higher state. Agent's belief about the environment malleability.\n",
    "\n",
    "        Returns:\n",
    "        - A new environment instance with updated transition probabilities.\n",
    "        '''\n",
    "        model = super().build_inner_model()\n",
    "        model.transition_table[0,:,:] = self.generate_dependent_transition(len(model.states), malleability)\n",
    "        return model\n",
    "    \n",
    "\n",
    "\n",
    "def demo():\n",
    "    env = Mindset_Environment(state_num=3, reward_baseline=2, reward_increment=1, malleability=1, session_length=100)\n",
    "    print(\"Reward Table:\\n\", env.reward_table)\n",
    "    print(\"Transition Table for dependent action:\\n\", env.transition_table[np.where(env.actions==\"A_dependent\")])\n",
    "    print(\"Transition Table for independent action:\\n\", env.transition_table[np.where(env.actions==\"A_independent\")])\n",
    "    model=env.build_inner_model(malleability=0.2)\n",
    "    print(\"Internal model:\\nstates:\\n\",model.states,\"\\nactions:\\n\",model.actions,\"\\nreward_table:\\n\",model.reward_table,\"\\ntransition_table:\\n\",model.transition_table)\n",
    "    print(f\"Parameters of environment:\\n{env.parameter}\")\n",
    "\n",
    "if __demo:\n",
    "    demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DP_Agent:\n",
    "    '''\n",
    "    A Dynamic Programming (DP) agent that interacts with the environment, figuring out an optimal policy based on a finite time horizon and discount rate.\n",
    "        policy: In a specific state, what action should be taken\n",
    "        optimal policy: policy that maximizes cumulative rewards given a planning length.\n",
    "\n",
    "    Attributes:\n",
    "    - env: The environment the agent is interacting with.\n",
    "    - time_horizon: The number of time steps the agent plans over.\n",
    "    - discount_rate: The subjective discount factor for future rewards.\n",
    "    - model: The agent's internal representation of the environment.\n",
    "\n",
    "    - expected_values_list: A 3D matrix storing the expected values for each state-action pair over time.\n",
    "    - policy: The optimal policy for each state and time step.\n",
    "    '''\n",
    "    def __init__(self, env, time_horizon=100, discount_rate=0.9):\n",
    "        '''\n",
    "        Initializes the DP Agent with the environment, time horizon, and discount rate.\n",
    "\n",
    "        Args:\n",
    "        - env: The environment the agent interacts with.\n",
    "        - time_horizon: Total number of steps the agent plans for. Length of the interaction session.\n",
    "        - discount_rate: Discount factor for future rewards.\n",
    "        '''\n",
    "        self.env = env\n",
    "        self.time_horizon = time_horizon\n",
    "        self.discount_rate = discount_rate\n",
    "        self.model = env\n",
    "\n",
    "        self.expected_values_list = None\n",
    "        self.policy = None\n",
    "\n",
    "        self.parameter={\n",
    "            \"time_horizon\": time_horizon,\n",
    "            \"discount_rate\": discount_rate\n",
    "        }\n",
    "    \n",
    "    def dynamic_programming(self, convergence_threshold=1e-5):\n",
    "        '''\n",
    "        Calculates the expected values using dynamic programming:\n",
    "        - The maximum depth of calculation is the time horizon of the agent, or the session length of the environment (if session length < time horizon).\n",
    "        - Expected values are stored in 'expected_values_list' attribution.\n",
    "\n",
    "        Args:\n",
    "        - convergence_threshold: The threshold for determining when the policy converges.\n",
    "        '''\n",
    "        states_num = len(self.model.states)\n",
    "        actions_num = len(self.model.actions)\n",
    "        transition_table = self.model.transition_table\n",
    "        reward_table = self.model.reward_table\n",
    "\n",
    "        # Initialize expected values and state values\n",
    "        expected_values_within_horizon = []\n",
    "        expected_values = np.zeros([actions_num, states_num])\n",
    "        state_values = np.max(expected_values, axis=0)\n",
    "\n",
    "        for planning_depth in range(self.time_horizon):\n",
    "            expected_values_old = expected_values.copy()# Copy one for us to check for convergence later.\n",
    "            \n",
    "            # Update expected values for each action-state pair\n",
    "            for a_i in range(actions_num):\n",
    "                for s_i in range(states_num):\n",
    "                    expected_values[a_i, s_i] = reward_table[a_i][s_i] + \\\n",
    "                        np.sum(transition_table[a_i, s_i, :] * state_values) * self.discount_rate\n",
    "            \n",
    "            # Store expected values for this planning depth\n",
    "            expected_values_within_horizon.append(expected_values.copy())\n",
    "            state_values = np.max(expected_values, axis=0)\n",
    "\n",
    "            # Check for convergence\n",
    "            if np.all(np.abs(expected_values - expected_values_old) < convergence_threshold):\n",
    "                #print(f\"Converged at planning depth {planning_depth + 1}\")\n",
    "                break\n",
    "        else:\n",
    "            pass#print(f\"Not converged, ended at planning depth {planning_depth + 1}\")\n",
    "\n",
    "        # Generate the 'expected_values_list' attribute by processing time_horizon and session_length. The resulting 'expected_values_list' should have length of session_length\n",
    "        if self.env.session_length > self.time_horizon:\n",
    "            # 1. Repeat the last time step enough times to cover the remaining length 2. Reverse the expected_values_within_horizon 3. Concatenate\n",
    "            self.expected_values_list = \\\n",
    "                [expected_values_within_horizon[-1]  for i in range(self.env.session_length - self.time_horizon)] +\\\n",
    "                expected_values_within_horizon[::-1]\n",
    "\n",
    "        else:\n",
    "            # 1. Slice the expected_values_list up to session_length 2. Reverse the result\n",
    "            self.expected_values_list = expected_values_within_horizon[:self.env.session_length][::-1]\n",
    "        self.expected_values_list=np.array(self.expected_values_list)\n",
    "        \n",
    "\n",
    "    def policy_readout(self):\n",
    "        '''\n",
    "        Reads out the optimal policy from the expected values ('expected_values_list').\n",
    "        Note: if there are multiple actions that achieve the highest expected value, the strategy will include tuples of all these tie actions.\n",
    "        '''\n",
    "        self.dynamic_programming()# The policy readout must be made after dynamic programming.\n",
    "\n",
    "        self.policy = []\n",
    "        for expected_values in self.expected_values_list:\n",
    "            state_values = np.max(expected_values, axis=0)\n",
    "            policy = [tuple(np.where(np.isclose(expected_values[:, s_i], state_value))[0]) \n",
    "                      for s_i, state_value in enumerate(state_values)]\n",
    "            self.policy.append(policy)\n",
    "\n",
    "def demo():\n",
    "    env = Mindset_Environment(malleability=0.9,session_length=10)\n",
    "    agent = DP_Agent(env,time_horizon=env.session_length,discount_rate=0.9)\n",
    "\n",
    "    agent.dynamic_programming()\n",
    "    agent.policy_readout()\n",
    "\n",
    "    print(\"Optimal policy:\\n\", agent.policy)\n",
    "    print(f\"\\nExpected values list:\\n {agent.expected_values_list}\")\n",
    "\n",
    "if __demo:\n",
    "    demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mindset_Agent(DP_Agent):\n",
    "    '''\n",
    "    A specialized DP Agent for mindset setup, with a belief about malleability.\n",
    "\n",
    "    Attributes:\n",
    "    - malleability_belief: The agent's belief about the malleability of the environment.\n",
    "    '''\n",
    "    def __init__(self, mindset_environment, malleability_belief=0.5, time_horizon=10, discount_rate=0.9):\n",
    "        '''\n",
    "        Initializes the mindset agent with the environment and its belief about malleability.\n",
    "\n",
    "        Args:\n",
    "        - env: The environment the agent interacts with. Note that this environment must be a Mindset_Environment instance\n",
    "        - malleability_belief: The agent's belief about environment probability. \n",
    "        - time_horizon: The number of steps the agent plans for.\n",
    "        - discount_rate: The discount factor for future rewards.\n",
    "        '''\n",
    "        super().__init__(mindset_environment, time_horizon, discount_rate)\n",
    "        # Initialize the malleability belief directly here\n",
    "        self._malleability_belief = malleability_belief\n",
    "\n",
    "        # Now use the setter to initialize the model\n",
    "        self.malleability_belief = malleability_belief\n",
    "        \n",
    "        self.model = mindset_environment.build_inner_model(malleability=malleability_belief)\n",
    "        \n",
    "        self.parameter={\n",
    "            \"malleability_belief\": malleability_belief,\n",
    "            \"time_horizon\": time_horizon,\n",
    "            \"discount_rate\": discount_rate\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def malleability_belief(self):\n",
    "        return self._malleability_belief\n",
    "\n",
    "    @malleability_belief.setter\n",
    "    def malleability_belief(self, new_belief):\n",
    "        '''\n",
    "        Setter for malleability_belief. Automatically updates the agent's 'model' and 'parameter' attributes\n",
    "        when 'malleability_belief' attributes changes.\n",
    "        '''\n",
    "        if new_belief != self._malleability_belief:\n",
    "            self._malleability_belief = new_belief\n",
    "            # Regenerate the model with the new malleability belief\n",
    "            self.model = self.env.build_inner_model(malleability=new_belief)\n",
    "            # Refresh the parameters\n",
    "            self.parameter[\"malleability_belief\"] = new_belief\n",
    "            print(f\"Malleability belief updated to {new_belief} and model refreshed.\")\n",
    "\n",
    "def demo():\n",
    "    env = Mindset_Environment(malleability=0.7,session_length=10) # Actually a malleable environment\n",
    "    print(f\"Environment transition:\\n{env.transition_table}\")\n",
    "    \n",
    "    print(\"Fixed mindset:\")\n",
    "    agent = Mindset_Agent(env, malleability_belief=0.1, time_horizon=env.session_length)#but the agent holds a fixed mindset\n",
    "\n",
    "    print(f\"FM agent belief transition:\\n{agent.model.transition_table}\")\n",
    "    \n",
    "    agent.dynamic_programming()\n",
    "    agent.policy_readout()\n",
    "    \n",
    "    print(\"Expected values:\\n\", agent.expected_values_list)\n",
    "    print(\"Optimal policy:\\n\", agent.policy)\n",
    "\n",
    "    \n",
    "    print(\"Growth mindset:\")\n",
    "    agent.malleability_belief=0.9# In contrast if the agent holds a growth mindset; change automatically\n",
    "    \n",
    "    print(f\"GM agent belief transition:\\n{agent.model.transition_table}\")\n",
    "    \n",
    "    agent.dynamic_programming()\n",
    "    agent.policy_readout()\n",
    "    \n",
    "    print(\"Expected values:\\n\", agent.expected_values_list)\n",
    "    print(\"Optimal policy:\\n\", agent.policy)\n",
    "\n",
    "    \n",
    "if __demo:\n",
    "    demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation:\n",
    "    '''\n",
    "    foldername: \n",
    "        <folder>\n",
    "        []: instance\n",
    "    \n",
    "    simulation_info:\n",
    "        <record>\n",
    "        []: simulation_id\n",
    "        (): category-field- value\n",
    "            simulation\n",
    "            agent\n",
    "            env\n",
    "\n",
    "    simulation_results:\n",
    "        <state, action, reward on time>\n",
    "        []: time_horizon, session_length, initial_state, session_index(noise), {agent algorithm, environment dynamics}\n",
    "        (): time\n",
    "\n",
    "    expected_values:\n",
    "        <expected_value>\n",
    "        []: time_horizon, session_index(noise), {agent algorithm, environment dynamics}\n",
    "        (): session_index, time, action, state\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "    '''\n",
    "    A simulation engine that simulate interaction between an environment and an agent, running a simulation\n",
    "    based on the agent's policy and the environment's dynamics.\n",
    "    It also store simulation results.\n",
    "    Attributes:\n",
    "    - env: A MDP environment (MDP_Environment) of the simulation.\n",
    "    - agent: The agent that will interact with the environment.\n",
    "    '''\n",
    "    def __init__(self, env, agent, folder_name):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        #When one instance is created, create a corresponding folder to store the result data.\n",
    "        self.folder_name=folder_name\n",
    "        self.create_results_folder()\n",
    "    \n",
    "    def run_one_session(self, initial_state = None):\n",
    "        '''\n",
    "        Runs one simulation session, starting from an initial state and simulating state transitions,\n",
    "        rewards, and actions based on the agent's policy and environment dynamics.\n",
    "\n",
    "        Args:\n",
    "        - initial_state: The starting state of the simulation.\n",
    "        '''\n",
    "        if initial_state is not None:\n",
    "            self.initial_state=initial_state\n",
    "        elif not hasattr(self, \"initial_state\"):\n",
    "            self.initial_state=input(\"Initial state {s1,s2,s3}:\")\n",
    "\n",
    "        self.agent.policy_readout()\n",
    "\n",
    "        session_length = self.env.session_length\n",
    "        s_i_path = np.full(session_length, None)\n",
    "        a_i_path = np.full(session_length, None)\n",
    "        r_path = np.full(session_length, None)\n",
    "\n",
    "        # Initial state index\n",
    "        s_i_path[0] = int(np.where(self.env.states == self.initial_state)[0][0])\n",
    "\n",
    "        for t in range(session_length):\n",
    "            # Choose action according to policy\n",
    "            a_i_path[t] = random.choice(self.agent.policy[t][s_i_path[t]])\n",
    "\n",
    "            # Yield reward for the state-action pair\n",
    "            r_path[t] = self.env.reward_table[a_i_path[t], s_i_path[t]]\n",
    "\n",
    "            # Transition to next state if not terminal\n",
    "            if t != session_length - 1:\n",
    "                transition_probabilities = self.env.transition_table[a_i_path[t], s_i_path[t], :]\n",
    "                state_indices = list(range(len(self.env.states)))\n",
    "                s_i_path[t + 1] = random.choices(state_indices, weights=transition_probabilities)[0]\n",
    "        \n",
    "        self.reward_path = r_path\n",
    "        # Re-encode actions and states from indexi\n",
    "        self.state_path = [self.env.states[int(s_i)] for s_i in s_i_path]\n",
    "        self.action_path = [self.env.actions[int(a_i)] for a_i in a_i_path]\n",
    "\n",
    "    def run_simulation(self, simulation_id, initial_state, simulation_num):\n",
    "        '''\n",
    "        Run multiple sessions, and store simulation trajectories, DP agent's expected values, simulation information\n",
    "        \n",
    "        Args:\n",
    "        - initial_state: The starting state of the simulation.\n",
    "        - simulation_num\n",
    "        '''\n",
    "        self.simulation_id = simulation_id\n",
    "\n",
    "        self.initial_state = initial_state\n",
    "        self.simulation_num = simulation_num\n",
    "        \n",
    "        for session_index in range(simulation_num):\n",
    "            self.session_index = session_index\n",
    "            self.run_one_session()\n",
    "            # Store simulation trajectories\n",
    "            self.save_simulation_trajectory()\n",
    "\n",
    "        # Store simulation info\n",
    "        self.save_simulation_info()\n",
    "        # Store expected values of dynamic programming\n",
    "        self.save_expected_values()\n",
    "\n",
    "    def save_simulation_trajectory(self, filename_prefix=\"simulation_results\"):\n",
    "        '''\n",
    "        Saves the simulation trajectory (time, action, state, reward) to a CSV file for further visualization and analysis in R.\n",
    "\n",
    "        Args:\n",
    "        - session_index: The simulation index for identification in the CSV file.\n",
    "        '''\n",
    "        # Get the current working directory to save the files\n",
    "        current_dir = os.getcwd()\n",
    "\n",
    "        # Save state path to CSV\n",
    "        state_path_file = os.path.join(current_dir, self.folder_name, f\"{filename_prefix}.csv\")\n",
    "        # Check if the file exists\n",
    "        file_exists = os.path.exists(state_path_file)\n",
    "\n",
    "\n",
    "        # Open CSV file to append data\n",
    "        with open(state_path_file, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            # Write the header only if the file does not exist\n",
    "            if not file_exists:\n",
    "                writer.writerow(['simulation_id', 'session_index', 'time', 'action', 'state', 'reward'])\n",
    "            # Write each time step\n",
    "            for t in range(self.env.session_length):\n",
    "                writer.writerow([self.simulation_id, self.session_index, t+1, self.action_path[t], self.state_path[t], self.reward_path[t]])\n",
    "        \n",
    "    def save_expected_values(self, filename_prefix=\"expected_values\"):\n",
    "        '''\n",
    "        Saves the expected values for each action and state to a CSV file.\n",
    "        '''\n",
    "        # Get the current working directory to save the files\n",
    "        current_dir = os.getcwd()\n",
    "\n",
    "        # Save state path to CSV\n",
    "        state_path_file = os.path.join(current_dir, self.folder_name, f\"{filename_prefix}.csv\")\n",
    "        # Check if the file exists\n",
    "        file_exists = os.path.exists(state_path_file)\n",
    "\n",
    "        expected_values_list = self.agent.expected_values_list\n",
    "\n",
    "        # Open CSV file to write expected values\n",
    "        with open(state_path_file, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            # Write the header only if the file does not exist\n",
    "            if not file_exists:\n",
    "                writer.writerow(['simulation_id', 'time', 'action', 'state', 'expected_value'])\n",
    "            \n",
    "            # Iterate over the time steps, actions, and states\n",
    "            for time_step in range(self.agent.time_horizon):\n",
    "                for action_index, action in enumerate(self.env.actions):\n",
    "                    for state_index, state in enumerate(self.env.states):\n",
    "                        \n",
    "                        expected_value = expected_values_list[time_step][action_index, state_index]\n",
    "                        writer.writerow([self.simulation_id, time_step+1, action, state, expected_value])\n",
    "\n",
    "    def save_simulation_info(self, filename_prefix=\"simulation_info\"):\n",
    "        '''\n",
    "        Saves the information about agent, environment, simulation configuration to a CSV file:\n",
    "        - agent:  malleability_belief, time_horizon, discount_rate\n",
    "        - environment: state_num, reward_baseline, reward_increment, malleability, session_length\n",
    "        - simulation: simulation_num, initial_state\n",
    "        '''\n",
    "        # Get the current working directory to save the files\n",
    "        current_dir = os.getcwd()\n",
    "\n",
    "        # Save state path to CSV\n",
    "        state_path_file = os.path.join(current_dir, self.folder_name, f\"{filename_prefix}.csv\")\n",
    "        # Check if the file exists\n",
    "        file_exists = os.path.exists(state_path_file)\n",
    "\n",
    "        # Open CSV file to write simulation information\n",
    "        with open(state_path_file, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            # Write the header only if the file does not exist\n",
    "            if not file_exists:\n",
    "                writer.writerow(['simulation_id','category', 'field', 'value'])\n",
    "\n",
    "            simulation_info=[\n",
    "                [\"simulation\",\"initial_state\",self.initial_state],\n",
    "                [\"simulation\",\"simulation_num\",self.simulation_num]\n",
    "            ]\n",
    "            for line in simulation_info:\n",
    "                writer.writerow([self.simulation_id]+line)\n",
    "            for field,value in self.agent.parameter.items():\n",
    "                writer.writerow([self.simulation_id]+[\"agent\",field,value])\n",
    "            for field,value in self.env.parameter.items():\n",
    "                writer.writerow([self.simulation_id]+[\"environment\",field,value])\n",
    "\n",
    "    def create_results_folder(self):\n",
    "        '''\n",
    "        Create a folder to store the simulation result data if it doesn't already exist.\n",
    "        '''\n",
    "        # Get the current working directory\n",
    "        current_dir = os.getcwd()\n",
    "\n",
    "        # Full path to the folder\n",
    "        folder_path = os.path.join(current_dir, self.folder_name)\n",
    "\n",
    "        # Create the folder if it doesn't exist\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        print(f\"Folder '{self.folder_name}' created (or already exists) at: {folder_path}\")\n",
    "\n",
    "def demo():\n",
    "    env = Mindset_Environment(malleability=0.7,session_length=10)\n",
    "    agent = Mindset_Agent(env, malleability_belief=0.1)\n",
    "    \n",
    "    simulation = Simulation(env, agent,\"Sep_16_pre\")\n",
    "\n",
    "    simulation.run_one_session()\n",
    "    print(\"state path:\\n\",simulation.state_path)\n",
    "    print(\"action path:\\n\",simulation.action_path)\n",
    "    print(\"reward path:\\n\",simulation.reward_path)\n",
    "\n",
    "    simulation.run_simulation(simulation_id=\"aaaa\", initial_state=\"s1\",simulation_num=10)\n",
    "    simulation.agent=Mindset_Agent(env, malleability_belief=0.9)\n",
    "    simulation.run_simulation(simulation_id=\"a66\", initial_state=\"s2\",simulation_num=3)\n",
    "    simulation.run_simulation(simulation_id=\"av5\", initial_state=\"s3\",simulation_num=10)\n",
    "\n",
    "if __demo:\n",
    "    demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a malleable environment\n",
    "env = Mindset_Environment(malleability=0.5,session_length=20) \n",
    "agent = Mindset_Agent(env, malleability_belief=0.5,  time_horizon = env.session_length, discount_rate = 0.9)\n",
    "\n",
    "simulation = Simulation(env, agent,\"Mindset_InitialState\")\n",
    "for mindset in [i/10 for i in range(0,11)]:\n",
    "    for s in env.states:\n",
    "        for horizon in [5,10,20]:\n",
    "            agent.malleability_belief=mindset\n",
    "            agent.time_horizon=horizon\n",
    "            simulation.run_simulation(simulation_id = \"_\".join([str(mindset),s,str(horizon)]), initial_state = s,simulation_num = 50)\n",
    "        #Variable\n",
    "        # - mindset {0,.1,.2,....,.9,1.0}\n",
    "        # - horizon:{5,10,20}\n",
    "        # - state {1,2,3}\n",
    "        #Static: \n",
    "        # - env: session_length:20; malleability=0.7\n",
    "        # - agent: time_horizon=session_length=50, discount_rate=0.9\n",
    "        # - simulation: 50\n",
    "        #Output\n",
    "        # - results (path): action, state, reward\n",
    "        # - expected_values\n",
    "        # - simulation_info\n",
    "\n",
    "\n",
    "\n",
    "simulation = Simulation(env, agent,\"Mindset_InitialState_EV\")\n",
    "for mindset in [i/10 for i in range(0,11)]:\n",
    "    for horizon in [5,10,20]:\n",
    "        agent.malleability_belief=mindset\n",
    "        agent.time_horizon=horizon\n",
    "        simulation.run_simulation(simulation_id = \"_\".join([str(mindset),str(horizon)]), initial_state = \"s1\",simulation_num = 50)\n",
    "        #Variable\n",
    "        # - mindset {0,.1,.2,....,.9,1.0}\n",
    "        # - horizon:{5,10,20}\n",
    "        # - state {1,2,3}\n",
    "        #Static: \n",
    "        # - env: session_length:20; malleability=0.7\n",
    "        # - agent: time_horizon=session_length=50, discount_rate=0.9\n",
    "        # - simulation: 50\n",
    "        #Output\n",
    "        # - results (path): action, state, reward\n",
    "        # - expected_values\n",
    "        # - simulation_info\n",
    "\n",
    "\n",
    "\n",
    "#argument: if the horizon is relatively short, mindset effect becomes strong; if it is too short, then mindset shall show same behavior. makes it difficult\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindset_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
